{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp.components import create_component_from_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"python:3.8-slim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_adapter(bucket_name: str, labels: list, output_dir_path: OutputPath()):\n",
    "\n",
    "    import io\n",
    "    import os\n",
    "    import dill\n",
    "    import boto3\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import transformers\n",
    "    from io import BytesIO\n",
    "    from contextlib import contextmanager\n",
    "    from tempfile import NamedTemporaryFile\n",
    "    from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "    class S3Adapter:\n",
    "        def __init__(self, labels):\n",
    "            self.labels = labels\n",
    "\n",
    "        def folder_exists_and_not_empty(self, bucket_name: str, key: str) -> bool:\n",
    "\n",
    "            if not key.endswith(\"/\"):\n",
    "                key = key + \"/\"\n",
    "            resp = self.s3_client.list_objects(\n",
    "                Bucket=bucket_name, Prefix=key, Delimiter=\"/\", MaxKeys=1\n",
    "            )\n",
    "            return \"Contents\" in resp\n",
    "\n",
    "        @contextmanager\n",
    "        def s3_fileobj(self, bucket_name, key):\n",
    "\n",
    "            \"\"\"\n",
    "            Yields a file object from the filename at {bucket}/{key}\n",
    "\n",
    "            Args:\n",
    "                bucket (str): Name of the S3 bucket where you model is stored\n",
    "                key (str): Relative path from the base of your bucket, including the filename and extension of the object to be retrieved.\n",
    "            \"\"\"\n",
    "\n",
    "            obj = self.s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "            yield BytesIO(obj[\"Body\"].read())\n",
    "\n",
    "        def get_data_from_s3(self, bucket_name, prefix, data_file):\n",
    "\n",
    "            data_object = self.s3_client.get_object(\n",
    "                Bucket=bucket_name, Key=os.path.join(prefix, data_file)\n",
    "            )[\"Body\"]\n",
    "            return data_object\n",
    "\n",
    "        def get_model_tokenizer_from_s3(self, bucket_name, prefix, model_name=\"model\"):\n",
    "\n",
    "            if not self.folder_exists_and_not_empty(\n",
    "                bucket_name=bucket_name, key=prefix\n",
    "            ):\n",
    "                model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    \"bert-base-uncased\", num_labels=len(self.labels)\n",
    "                )\n",
    "                tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "                    \"bert-base-uncased\"\n",
    "                )\n",
    "                self.save_model_tokenizer_into_s3(\n",
    "                    bucket_name=bucket_name,\n",
    "                    prefix=prefix,\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                )\n",
    "                return model, tokenizer\n",
    "\n",
    "            else:\n",
    "                tempfile = NamedTemporaryFile()\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/{model_name}.bin\"\n",
    "                ) as f:\n",
    "                    tempfile.write(f.read())\n",
    "\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/config.json\"\n",
    "                ) as f:\n",
    "                    dict_data = json.load(f)\n",
    "                    print(type(dict_data))\n",
    "                    config = PretrainedConfig.from_dict(dict_data)\n",
    "\n",
    "                #                 model = PreTrainedModel.from_pretrained(tempfile.name, config=config)\n",
    "                model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    tempfile.name, config=config\n",
    "                )\n",
    "                tokenizer = self.get_tokenizer_from_s3(bucket_name, prefix)\n",
    "                return model, tokenizer\n",
    "\n",
    "        def get_tokenizer_from_s3(self, bucket_name, prefix):\n",
    "\n",
    "            tempfile = NamedTemporaryFile()\n",
    "            with self.s3_fileobj(\n",
    "                bucket_name=bucket_name, key=f\"{prefix}/vocab.txt\"\n",
    "            ) as f:\n",
    "                tempfile.write(f.read())\n",
    "            tokenizer = transformers.BertTokenizer.from_pretrained(tempfile.name)\n",
    "            return tokenizer\n",
    "\n",
    "        def save_model_tokenizer_into_s3(\n",
    "            self, bucket_name, prefix, model, model_name, tokenizer=None\n",
    "        ):\n",
    "            buffer = io.BytesIO()\n",
    "            config_string = model.config.to_json_string()\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name, Key=f\"{prefix}/config.json\", Body=config_string\n",
    "            )\n",
    "            torch.save(model.state_dict(), buffer)\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=f\"{prefix}/{model_name}.bin\",\n",
    "                Body=buffer.getvalue(),\n",
    "            )\n",
    "            if tokenizer:\n",
    "                vocab_string = \"\"\n",
    "                for token, token_id in tokenizer.vocab.items():\n",
    "                    vocab_string = vocab_string + token + \"\\n\"\n",
    "\n",
    "                vocab_string = vocab_string.strip()\n",
    "                self.s3_client.put_object(\n",
    "                    Bucket=bucket_name, Key=f\"{prefix}/vocab.txt\", Body=vocab_string\n",
    "                )\n",
    "\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    s3_adpater_obj = S3Adapter(labels=labels)\n",
    "    s3_pickle_path = os.path.join(output_dir_path, \"s3_adapter.pkl\")\n",
    "    with open(s3_pickle_path, \"wb\") as f:\n",
    "        dill.dump(s3_adpater_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(\n",
    "    input_dir_path: InputPath(), labels: list, mode: str, output_dir_path: OutputPath()\n",
    "):\n",
    "\n",
    "    import os\n",
    "    import os\n",
    "    import dill\n",
    "    import torch\n",
    "    import transformers\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class LabelDataset(Dataset):\n",
    "        \"\"\"\n",
    "\n",
    "        LabelDataset class - to load the dataset used the __getitem__ fashion supported by the Pytorch.\n",
    "        The loader supports the JSON and the csv format for parsing the input to the network.\n",
    "            :param mode: mode of Label classifier model (train, eval or serve)\n",
    "            :param text: input text for train, eval and serve components\n",
    "            :param label: output label for train and eval components\n",
    "            :param func_test: True for functional testing of package\n",
    "            :param tokenizer: tokenizer from huggingface library\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, labels, mode=\"serve\", max_length=512, text=None, label=None):\n",
    "            self.mode = mode\n",
    "            self.labels = labels\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __getitem__(self, item):\n",
    "            \"\"\"\n",
    "\n",
    "            Returns tokenized tensors for text and label(if mode is train or eval) for the given index.\n",
    "                :param: item: index to fetch the data.\n",
    "                :returns dict: dictionary of tensors containing input_ids, attention_mask,\n",
    "                               token_type_ids and label(output if mode is train or eval)\n",
    "\n",
    "            \"\"\"\n",
    "            if self.mode in [\"train_eval\", \"train\", \"eval\"]:\n",
    "                text = str(self.text[item])\n",
    "                processed_text = \" \".join(text.split())\n",
    "                label = self.label[item]\n",
    "                inputs = self.tokenizer.encode_plus(\n",
    "                    processed_text,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                padding_length = self.max_length - len(inputs.get(\"input_ids\"))\n",
    "                input_ids = inputs.get(\"input_ids\") + ([0] * padding_length)\n",
    "                attention_mask = inputs.get(\"attention_mask\") + ([0] * padding_length)\n",
    "                token_type_ids = inputs.get(\"token_type_ids\") + ([0] * padding_length)\n",
    "                return {\n",
    "                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                    \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "                    \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                    \"label\": torch.tensor(\n",
    "                        self.labels.index(label.strip()), dtype=torch.long\n",
    "                    ),\n",
    "                }\n",
    "            elif self.mode == \"serve\":\n",
    "                text = str(self.text[item])\n",
    "                text = \" \".join(text.split())\n",
    "                inputs = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                padding_length = self.max_length - len(inputs.get(\"input_ids\"))\n",
    "                input_ids = inputs.get(\"input_ids\") + ([0] * padding_length)\n",
    "                attention_mask = inputs.get(\"attention_mask\") + ([0] * padding_length)\n",
    "                token_type_ids = inputs.get(\"token_type_ids\") + ([0] * padding_length)\n",
    "                return {\n",
    "                    \"text\": text,\n",
    "                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                    \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "                    \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                }\n",
    "\n",
    "        def __len__(self):\n",
    "            \"\"\"\n",
    "\n",
    "            Returns length of dataset\n",
    "                :returns int: length of dataset\n",
    "\n",
    "            \"\"\"\n",
    "            return len(self.text)\n",
    "\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    with open(os.path.join(input_dir_path, \"s3_adapter.pkl\"), \"rb\") as f:\n",
    "        s3_adapter_obj = dill.load(f)\n",
    "    with open(os.path.join(output_dir_path, \"s3_adapter.pkl\"), \"wb\") as f:\n",
    "        dill.dump(s3_adapter_obj, f)\n",
    "\n",
    "    dataset_object = LabelDataset(mode=mode, labels=labels)\n",
    "    # Writing LabelBackbone class into model.pkl file\n",
    "    with open(os.path.join(output_dir_path, \"dataset.pkl\"), \"wb\") as f:\n",
    "        dill.dump(dataset_object, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_dir_path: InputPath(),\n",
    "    bucket_name: str,\n",
    "    prefix: str,\n",
    "    data_file: str,\n",
    "    labels: list,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    device: str,\n",
    "    output_dir_path: OutputPath(),\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Training method for Label Classifier. Saves the model after training of model is completed\n",
    "        :param num_epochs: Number of epochs for training purpose\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import io\n",
    "    import os\n",
    "    import dill\n",
    "    import json\n",
    "    import boto3\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import transformers\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    from transformers import AdamW\n",
    "    import torch.nn.functional as f\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    class LabelBackbone(nn.Module):\n",
    "        \"\"\"\n",
    "\n",
    "        LabelBackbone - Backbone model class for Label Classifier\n",
    "            :param model: Transformer model name from config.TRANSFORMER_MODEL_LIST or saved transformer model name\n",
    "            :param tokenizer: tokenizer from huggingface library\n",
    "            :returns Object of LabelBackbone Model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.drop_out = nn.Dropout(p=0.3)\n",
    "\n",
    "        def forward(self, **kwargs):\n",
    "            return self.model(**kwargs)\n",
    "            #             final_output = self.linear(self.drop_out(po))\n",
    "            return final_output\n",
    "\n",
    "    with open(os.path.join(input_dir_path, \"s3_adapter.pkl\"), \"rb\") as f:\n",
    "        s3_adapter_obj = dill.load(f)\n",
    "\n",
    "    setattr(s3_adapter_obj, \"s3_client\", s3_client)\n",
    "    pretrained_model, tokenizer = s3_adapter_obj.get_model_tokenizer_from_s3(\n",
    "        bucket_name=bucket_name, prefix=f\"{prefix}/pretrained\"\n",
    "    )\n",
    "\n",
    "    #     pretrained_model = s3_adapter_obj.get_model_from_s3(\n",
    "    #         bucket_name=bucket_name, prefix=f\"{prefix}/pretrained_model\"\n",
    "    #     )\n",
    "    backbone_model = LabelBackbone(model=pretrained_model)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = AdamW(backbone_model.parameters(), lr=3e-5)\n",
    "\n",
    "    dataframe = pd.read_csv(\n",
    "        s3_adapter_obj.get_data_from_s3(\n",
    "            bucket_name=bucket_name, prefix=prefix, data_file=data_file\n",
    "        )\n",
    "    )\n",
    "    print(dataframe.head())\n",
    "    text = dataframe.loc[:, \"text\"]\n",
    "    label = dataframe.loc[:, \"label\"]\n",
    "\n",
    "    with open(os.path.join(input_dir_path, \"dataset.pkl\"), \"rb\") as f:\n",
    "        train_dataset = dill.load(f)\n",
    "    setattr(train_dataset, \"text\", text)\n",
    "    setattr(train_dataset, \"label\", label)\n",
    "    setattr(train_dataset, \"mode\", \"train\")\n",
    "    setattr(train_dataset, \"tokenizer\", tokenizer)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    try:\n",
    "        backbone_model.to(device)\n",
    "        backbone_model.train()  # Call the train method from the nn.Module base class\n",
    "        print(\"Starting the Training Loop ..\")  # Training loop start\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            train_accuracy = 0\n",
    "            print(f\"[INFO] Epoch {epoch + 1} Started..\")\n",
    "            for index, batch in tqdm(enumerate(train_data_loader)):\n",
    "                print(\n",
    "                    f\"[INFO] [TRAINING] Epoch {epoch + 1} Iteration {index + 1} Running..\"\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "                label = batch[\"label\"].to(device).to(device)\n",
    "                output = backbone_model(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask, labels=label\n",
    "                )\n",
    "                #                 loss = loss_function(output, label)\n",
    "                loss = output[0]\n",
    "                print(output[1])\n",
    "                print(\"loss\", loss, type(loss))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = train_loss + loss.item()\n",
    "                _, hypothesis = torch.max(output[1], dim=1)\n",
    "                train_accuracy = (\n",
    "                    train_accuracy\n",
    "                    + torch.sum(torch.tensor(hypothesis == label)).item()\n",
    "                )\n",
    "            train_accuracy = train_accuracy / (\n",
    "                len(self.data_loader[0]) * batch_size\n",
    "            )\n",
    "            accuracy.append(train_accuracy)\n",
    "            train_loss = train_loss / (\n",
    "                len(self.data_loader[0]) * batch_size\n",
    "            )\n",
    "            train_st = f\"Training Loss: {train_loss} Train Accuracy: {train_accuracy}\"\n",
    "            print(f\"Epoch: {epoch+1} {train_st}\")\n",
    "\n",
    "        print(\"Model has been successfully built..\")\n",
    "        # utils_tools.save_model_bin(model_name=config.MARKER_CLASSIFIER, model=self.model)\n",
    "        #         accuracy = sum(accuracy) / len(accuracy)\n",
    "        s3_adapter_obj.save_model_tokenizer_into_s3(\n",
    "            bucket_name=bucket_name,\n",
    "            prefix=f\"{prefix}/finetuned\",\n",
    "            model=backbone_model.model,\n",
    "        )\n",
    "\n",
    "    except (RuntimeError, MemoryError, ValueError, TypeError) as e:\n",
    "        print(\"Training Exception Occurred\")\n",
    "        raise RuntimeError(\"Training Exception Occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_adapter_comp = create_component_from_func(\n",
    "    s3_adapter,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"boto3\", \"dill\", \"pandas\", \"transformers\", \"torch\"],\n",
    ")\n",
    "dataset_comp = create_component_from_func(\n",
    "    create_data_loader,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"torch\", \"dill\", \"transformers\"],\n",
    ")\n",
    "train_comp = create_component_from_func(\n",
    "    train,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"torch\", \"pandas\", \"transformers\", \"dill\", \"boto3\", \"numpy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"label-classifier-training-pipeline\")\n",
    "def model_pipeline(labels: list):\n",
    "    mode = \"train\"\n",
    "    prefix = \"kf-label-classifier\"\n",
    "    bucket_name = \"mlops-kubeflow\"\n",
    "    labels = [\n",
    "        \"Action with Deadline\",\n",
    "        \"Announcement\",\n",
    "        \"Appreciation\",\n",
    "        \"Action\",\n",
    "        \"Others\",\n",
    "    ]\n",
    "    s3_adapter_task = s3_adapter_comp(bucket_name=bucket_name, labels=labels)\n",
    "    dataset_task = dataset_comp(\n",
    "        input_dir=s3_adapter_task.output, labels=labels, mode=mode\n",
    "    )\n",
    "    train_task = train_comp(\n",
    "        input_dir=dataset_task.output,\n",
    "        bucket_name=bucket_name,\n",
    "        prefix=prefix,\n",
    "        data_file=\"label.csv\",\n",
    "        labels=labels,\n",
    "        num_epochs=10,\n",
    "        batch_size=2,\n",
    "        device=\"cpu\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-workos-slvr.anthem.com/pipeline/#/experiments/details/bc9aff0e-45e7-4459-81a1-5af693dfcdfb\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-workos-slvr.anthem.com/pipeline/#/experiments/details/bc9aff0e-45e7-4459-81a1-5af693dfcdfb\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow-workos-slvr.anthem.com/pipeline/#/runs/details/7a807322-4a85-4181-9a9c-03c1e57ff307\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=7a807322-4a85-4181-9a9c-03c1e57ff307)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"label_classifier\"\n",
    "HOST = \"https://kubeflow-workos-slvr.anthem.com\"\n",
    "namespace = \"chaluvadi-avinash\"\n",
    "session_cookie = \"MTY0NTQyNTI5M3xOd3dBTkRkVVRUTkxTa3RVVkVnM1MxTkhUVTFKVmxCQ05WQXpVVTFFVUVSSE0wdE9RVXRGTlROSVRsQkhRVU5NV2xkWU4wRktSRkU9fDyBx8kiey-TPSimHemde3ySqxXCVyP_8OhlH06bBMu3\"\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\",\n",
    "    cookies=f\"authservice_session={session_cookie}\",\n",
    "    namespace=namespace,\n",
    "    ssl_ca_cert=\"./root.pem\",\n",
    ")\n",
    "experiment = client.create_experiment(name=EXPERIMENT_NAME, namespace=namespace)\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=model_pipeline,\n",
    "    arguments={},\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=namespace,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_dir_path: InputPath(), data_file:str, num_epochs:int, batch_size:int, device:str, label_string:str, model_path:str, tokenizer_path:str, output_dir_path: OutputPath()):\n",
    "    \"\"\"\n",
    "    \n",
    "        Training method for Label Classifier. Saves the model after training of model is completed\n",
    "            :param num_epochs: Number of epochs for training purpose\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    import dill\n",
    "    import json\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    from transformers import AdamW\n",
    "    import torch.nn.functional as f\n",
    "    \n",
    "    \n",
    "    dataframe = df.read_csv(os.path.join(input_dir_path, data_file_name))\n",
    "    model_pcikle_path = os.path.join(input_dir_path, \"model.pkl\")\n",
    "    with open(model_pcikle_path, \"rb\") as f:\n",
    "        model = dill.load(f)\n",
    "    optimizer = AdamW(self.model.parameters(), lr=3e-5)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    setattr(model, \"model\", transformers.BertForSequenceClassification(model_path))\n",
    "    \n",
    "#     def create_batches(dataframe):\n",
    "#         tokenizer = transformers.BertTokenizer.from_pretrained(tokenizer_path)\n",
    "#         labels = json.loads(label_string)\n",
    "#         train_data = []\n",
    "#         for index in range(0, len(df), batch_size):\n",
    "#             batch_dict = {}\n",
    "#             end_index = index + batch_size\n",
    "#             text = df.loc[index: end_index, \"text\"].values\n",
    "#             marker = df.loc[index: end_index, \"marker\"].values.to_list()\n",
    "#             inputs = tokenizer.batch_encode_plus(text, max_length=512, padding=\"max_length\", truncation=True)\n",
    "#             batch_dict[\"input_ids\"] = torch.tensor(inputs.get(\"input_ids\"), dtype=torch.long)\n",
    "#             batch_dict[\"attention_mask\"] = torch.tensor(inputs.get(\"attention_mask\"), dtype=torch.long)\n",
    "#             batch_dict[\"token_type_ids\"] = torch.tensor(inputs.get(\"token_type_ids\"), dtype=torch.long)\n",
    "#             batch_data[\"marker\"] = torch.tensor(map(lambda x: labels.index(x.title()),marker), dtype=torch.long)\n",
    "#             train_data.append(batch_dict)\n",
    "#         return train_data\n",
    "    \n",
    "    train_data = create_batches(dataframe=dataframe)\n",
    "    \n",
    "    try:\n",
    "        model.to(device)\n",
    "        model.train()  # Call the train method from the nn.Module base class\n",
    "        print(\"Starting the Training Loop ..\")  # Training loop start\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            train_accuracy = 0\n",
    "            print(f\"[INFO] Epoch {epoch + 1} Started..\")\n",
    "            for index, batch in tqdm(enumerate(train_data)):\n",
    "                print(\n",
    "                    f\"[INFO] [TRAINING] Epoch {epoch + 1} Iteration {index + 1} Running..\"\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "                marker = batch[\"marker\"].to(self.device).to(device)\n",
    "                output = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                )\n",
    "                loss = loss_function(output, marker)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = train_loss + loss.item()\n",
    "                _, hypothesis = torch.max(output, dim=1)\n",
    "                train_accuracy = (\n",
    "                    train_accuracy\n",
    "                    + torch.sum(torch.tensor(hypothesis == marker)).item()\n",
    "                )\n",
    "            train_accuracy = train_accuracy / (\n",
    "                len(self.data_loader[0])\n",
    "                * config.params.get(\"train\").get(\"batch_size\")\n",
    "            )\n",
    "            accuracy.append(train_accuracy)\n",
    "            train_loss = train_loss / (\n",
    "                len(self.data_loader[0])\n",
    "                * config.params.get(\"train\").get(\"batch_size\")\n",
    "            )\n",
    "            train_st = (\n",
    "                f\"Training Loss: {train_loss} Train Accuracy: {train_accuracy}\"\n",
    "            )\n",
    "            print(f\"Epoch: {epoch+1} {train_st}\")\n",
    "\n",
    "        print(\"Model has been successfully built..\")\n",
    "        # utils_tools.save_model_bin(model_name=config.MARKER_CLASSIFIER, model=self.model)\n",
    "        accuracy = sum(accuracy) / len(accuracy)\n",
    "\n",
    "        ## save the model chackpoint to minio\n",
    "            \n",
    "    except (RuntimeError, MemoryError, ValueError, TypeError) as e:\n",
    "        print(\"Training Exception Occurred\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"mlops-kubeflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3client.download_file(Bucket='mlops-kubeflow',Key='pipeline_parameters_and_metrics.ipynb',Filename='C:\\\\Users\\\\AG98087\\\\Downloads\\\\lakshmi_narayana.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': 'F0FK6N4FRHDK68T2',\n",
       "   'HostId': 'XxQMbZOzA2p8NDIZuUrnqGOpwkinYXfWq7Z6SUkJ9BmYs7ftxqyzorWNM3WbwKJnn6eqIYm6KEw=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'XxQMbZOzA2p8NDIZuUrnqGOpwkinYXfWq7Z6SUkJ9BmYs7ftxqyzorWNM3WbwKJnn6eqIYm6KEw=',\n",
       "    'x-amz-request-id': 'F0FK6N4FRHDK68T2',\n",
       "    'date': 'Mon, 21 Feb 2022 10:05:25 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'kf-label-classifier/label.csv',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': '20fd8vej4dPRuV5VzkO81F4h3BpnspuX'},\n",
       "   {'Key': 'kf-label-classifier/pretrained/model.bin',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': '5euWEIxLxnYY4vYoqskk7IeaQO56yPxE'},\n",
       "   {'Key': 'kf-label-classifier/pretrained/vocab.txt',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'hkpYSPaxjHivAQaEKxNlKNVDCusTDdru'},\n",
       "   {'Key': 'kf-label-classifier/pretrained/config.json',\n",
       "    'DeleteMarker': True,\n",
       "    'DeleteMarkerVersionId': 'CSnZKl1fnY2aWQZLzmbsOozqV_aAMzEW'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "bucket = s3.Bucket(\"mlops-kubeflow\")\n",
    "bucket.objects.filter(Prefix=\"kf-label-classifier\").delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3.Bucket(\"mlops-kubeflow\").upload_file(\"label.csv\", \"kf-label-classifier/label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ZZ3PHH5EF9N96EJN',\n",
       "  'HostId': 'oJEwzUZLewzk1AvafDAHVojRK9YCpqO/psJrPXzR/ju6kNVPlWs6zP5337PsQb52Y6P2l3DvYS0=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'oJEwzUZLewzk1AvafDAHVojRK9YCpqO/psJrPXzR/ju6kNVPlWs6zP5337PsQb52Y6P2l3DvYS0=',\n",
       "   'x-amz-request-id': 'ZZ3PHH5EF9N96EJN',\n",
       "   'date': 'Fri, 18 Feb 2022 15:32:43 GMT',\n",
       "   'x-amz-version-id': 'dWv30nE68SVSxt1XrRiNkaLvMlF_AY4X',\n",
       "   'x-amz-expiration': 'expiry-date=\"Sun, 19 Feb 2023 00:00:00 GMT\", rule-id=\"tf-s3-lifecycle-20220210124446674200000002\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"97cddd635cef02b3ceaf25641f9b2eee\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 1},\n",
       " 'Expiration': 'expiry-date=\"Sun, 19 Feb 2023 00:00:00 GMT\", rule-id=\"tf-s3-lifecycle-20220210124446674200000002\"',\n",
       " 'ETag': '\"97cddd635cef02b3ceaf25641f9b2eee\"',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'VersionId': 'dWv30nE68SVSxt1XrRiNkaLvMlF_AY4X'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "st = \"Avinash\"\n",
    "st = st.encode()\n",
    "s3.put_object(Bucket=\"mlops-kubeflow\", Key=\"kf-label-classifier/test.bin\", Body=st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ceccfe346834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ms3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m obj = s3.get_object(Bucket=\"mlops-kubeflow\", Key=\"test-label-classifier/test.csv\")[\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"Body\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=\"mlops-kubeflow\", Key=\"test-label-classifier/test.csv\")[\n",
    "    \"Body\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Legato takes to the field on 27th and 28th of ...</td>\n",
       "      <td>Announcement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your great work has earned you a vacation</td>\n",
       "      <td>Appreciation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess Kajari told you to watch those videos...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what no confusion for per week 2 weeks scripts...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are full of constructive suggestions</td>\n",
       "      <td>Appreciation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>Move it to tested then only I will move it to ...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>So this particular thing I think this should b...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>I am busy with UI and UX design and review as ...</td>\n",
       "      <td>Action with Deadline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>Uh I am putting it in in-review but. Okay you ...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>We are pleased to welcome Sudha Munireddy as o...</td>\n",
       "      <td>Announcement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>632 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text                 label\n",
       "0    Legato takes to the field on 27th and 28th of ...          Announcement\n",
       "1            Your great work has earned you a vacation          Appreciation\n",
       "2     I guess Kajari told you to watch those videos...                Action\n",
       "3    what no confusion for per week 2 weeks scripts...                Others\n",
       "4             You are full of constructive suggestions          Appreciation\n",
       "..                                                 ...                   ...\n",
       "627  Move it to tested then only I will move it to ...               Action \n",
       "628  So this particular thing I think this should b...                Others\n",
       "629  I am busy with UI and UX design and review as ...  Action with Deadline\n",
       "630  Uh I am putting it in in-review but. Okay you ...                Action\n",
       "631  We are pleased to welcome Sudha Munireddy as o...          Announcement\n",
       "\n",
       "[632 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(obj)\n",
    "# ByetsIO(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "def test():\n",
    "    class LabelBackbone(nn.Module):\n",
    "        \"\"\"\n",
    "\n",
    "        LabelBackbone - Backbone model class for Label Classifier\n",
    "            :param model: Transformer model name from config.TRANSFORMER_MODEL_LIST or saved transformer model name\n",
    "            :param tokenizer: tokenizer from huggingface library\n",
    "            :returns Object of LabelBackbone Model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "    #             self.model = transformers.BertForSequenceClassification(model_path)\n",
    "            self.drop_out = nn.Dropout(p=0.3)\n",
    "            self.linear = nn.Linear(768, 3)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "            _, po = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                return_dict=False,\n",
    "            )\n",
    "            final_output = self.linear(self.drop_out(po))\n",
    "            return final_output\n",
    "\n",
    "    backbone_model = LabelBackbone()\n",
    "    print(\"**********\", LabelBackbone.__name__)\n",
    "    # Writing LabelBackbone class into model.pkl file\n",
    "#     with open(\"test.pkl\", \"wb\") as f:\n",
    "#         dill.dump(backbone_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Avinash'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.pkl\", \"rb\") as f:\n",
    "    ob = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "could not extract source code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dill/source.py:354\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object, alias, lstrip, enclosing, force, builtin)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# get source lines; if fail, try to 'force' an import\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# fails for builtins, and other assorted object types\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menclosing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menclosing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mIOError\u001b[39;00m): \u001b[38;5;66;03m# failed to get source, resort to import hooks\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force: \u001b[38;5;66;03m# don't try to get types that findsource can't get\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dill/source.py:325\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object, lstrip, enclosing)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsourcelines\u001b[39m(\u001b[38;5;28mobject\u001b[39m, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, enclosing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    Interactively-defined objects refer to lines in the interpreter's history.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    If lstrip=True, ensure there is no indentation in the first line of code.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    If enclosing=True, then also return any enclosing code.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m     code, n \u001b[38;5;241m=\u001b[39m \u001b[43mgetblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstrip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstrip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menclosing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menclosing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m code[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dill/source.py:251\u001b[0m, in \u001b[0;36mgetblocks\u001b[0;34m(object, lstrip, enclosing, locate)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetblocks\u001b[39m(\u001b[38;5;28mobject\u001b[39m, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, enclosing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, locate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    Interactively-defined objects refer to lines in the interpreter's history.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    DEPRECATED: use 'getsourcelines' instead\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lstrip: lines \u001b[38;5;241m=\u001b[39m _outdent(lines)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/dill/source.py:154\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    151\u001b[0m         lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcould not extract source code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m#FIXME: all below may fail if exec used (i.e. exec('f = lambda x:x') )\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[0;31mOSError\u001b[0m: could not extract source code"
     ]
    }
   ],
   "source": [
    "dill.source.getsource(ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>marker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeah so this particular login screen code you ...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah I have shared with you and divesh both of...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yeah divesh please go ahead. This piece of cod...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello yes divesh your voice is not audible. He...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think. Still some noise is happening. This p...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Okay yeah yeah suresh anything pending from yo...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Um, uh, I don't think so himanshu so actually ...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Wednesday is our demo so today by end of the d...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Thank you guys so uh whatever you guys have sh...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>thank you himanshu thank you himanshu thank yo...</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  marker\n",
       "0    Yeah so this particular login screen code you ...  Others\n",
       "1    Yeah I have shared with you and divesh both of...  Others\n",
       "2    Yeah divesh please go ahead. This piece of cod...  Others\n",
       "3    Hello yes divesh your voice is not audible. He...  Others\n",
       "4    I think. Still some noise is happening. This p...  Others\n",
       "..                                                 ...     ...\n",
       "275  Okay yeah yeah suresh anything pending from yo...  Others\n",
       "276  Um, uh, I don't think so himanshu so actually ...  Others\n",
       "277  Wednesday is our demo so today by end of the d...  Others\n",
       "278  Thank you guys so uh whatever you guys have sh...  Others\n",
       "279  thank you himanshu thank you himanshu thank yo...  Others\n",
       "\n",
       "[280 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "pd.read_csv(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.BytesIO"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import torch\n",
    "by = BytesIO(obj[\"Body\"].read())\n",
    "type(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: not a ZIP archive",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_serialization.py:163\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files)\u001b[0m\n\u001b[1;32m    161\u001b[0m     cpp_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mimport_ir_module(cu, \u001b[38;5;28mstr\u001b[39m(f), map_location, _extra_files)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     cpp_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_ir_module_from_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_extra_files\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# TODO: Pretty sure this approach loses ConstSequential status and such\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_cpp_module(cpp_module)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: not a ZIP archive"
     ]
    }
   ],
   "source": [
    "torch.jit.load(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "# tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# torch.load(\".bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.batch_encode_plus(np.array([\"Hi How are you\", \"Can we connect ?\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 7632, 2129, 2024, 2017, 102], [101, 2064, 2057, 7532, 1029, 102]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 7632, 2129,  ...,    0,    0,    0],\n",
       "        [ 101, 2064, 2057,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(inputs[\"input_ids\"], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager \n",
    "from io import BytesIO \n",
    "import torch\n",
    "from tempfile import NamedTemporaryFile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager \n",
    "def s3_fileobj(bucket, key): \n",
    "    \"\"\"\n",
    "    Yields a file object from the filename at {bucket}/{key}\n",
    "\n",
    "    Args:\n",
    "        bucket (str): Name of the S3 bucket where you model is stored\n",
    "        key (str): Relative path from the base of your bucket, including the filename and extension of the object to be retrieved.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\") \n",
    "    obj = s3.get_object(Bucket=bucket, Key=key) \n",
    "    yield BytesIO(obj[\"Body\"].read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(bucket, path_to_model, model_name='pytorch_model'):\n",
    "    \"\"\"\n",
    "    Load a model at the given S3 path. It is assumed that your model is stored at the key:\n",
    "\n",
    "        '{path_to_model}/{model_name}.bin'\n",
    "\n",
    "    and that a config has also been generated at the same path named:\n",
    "\n",
    "        f'{path_to_model}/config.json'\n",
    "\n",
    "    \"\"\"\n",
    "    tempfile = NamedTemporaryFile() \n",
    "    with s3_fileobj(bucket, path_to_model) as f: \n",
    "        tempfile.write(f.read()) \n",
    " \n",
    "#     with s3_fileobj(bucket, f'{path_to_model}/config.json') as f: \n",
    "#         dict_data = json.load(f) \n",
    "#         config = PretrainedConfig.from_dict(dict_data) \n",
    " \n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(tempfile.name)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7f6f5e5a3670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7f6f5e5a3670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/std.py\", line 1147, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "model = load_model('mlops-kubeflow', 'kf-label-classifier/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    bucket_name: str,\n",
    "    prefix: str,\n",
    "    data_file: str,\n",
    "    labels: list,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    device: str,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Training method for Label Classifier. Saves the model after training of model is completed\n",
    "        :param num_epochs: Number of epochs for training purpose\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import io\n",
    "    import os\n",
    "    import dill\n",
    "    import json\n",
    "    import boto3\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import transformers\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    from transformers import AdamW\n",
    "    import torch.nn.functional as f\n",
    "    from torch.utils.data import DataLoader\n",
    "    from contextlib import contextmanager\n",
    "    from tempfile import NamedTemporaryFile\n",
    "    from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    #     os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    class LabelBackbone(nn.Module):\n",
    "        \"\"\"\n",
    "\n",
    "        LabelBackbone - Backbone model class for Label Classifier\n",
    "            :param model: Transformer model name from config.TRANSFORMER_MODEL_LIST or saved transformer model name\n",
    "            :param tokenizer: tokenizer from huggingface library\n",
    "            :returns Object of LabelBackbone Model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.drop_out = nn.Dropout(p=0.3)\n",
    "\n",
    "        def forward(self, **kwargs):\n",
    "            return self.model(**kwargs)\n",
    "            #             final_output = self.linear(self.drop_out(po))\n",
    "            return final_output\n",
    "\n",
    "    class S3Adapter:\n",
    "        def __init__(self, labels, client):\n",
    "            self.s3_client = client\n",
    "            self.labels = labels\n",
    "\n",
    "        def folder_exists_and_not_empty(self, bucket_name: str, key: str) -> bool:\n",
    "\n",
    "            if not key.endswith(\"/\"):\n",
    "                key = key + \"/\"\n",
    "            resp = self.s3_client.list_objects(\n",
    "                Bucket=bucket_name, Prefix=key, Delimiter=\"/\", MaxKeys=1\n",
    "            )\n",
    "            return \"Contents\" in resp\n",
    "\n",
    "        @contextmanager\n",
    "        def s3_fileobj(self, bucket_name, key):\n",
    "\n",
    "            \"\"\"\n",
    "            Yields a file object from the filename at {bucket}/{key}\n",
    "\n",
    "            Args:\n",
    "                bucket (str): Name of the S3 bucket where you model is stored\n",
    "                key (str): Relative path from the base of your bucket, including the filename and extension of the object to be retrieved.\n",
    "            \"\"\"\n",
    "\n",
    "            obj = self.s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "            yield io.BytesIO(obj[\"Body\"].read())\n",
    "\n",
    "        def get_data_from_s3(self, bucket_name, prefix, data_file):\n",
    "\n",
    "            data_object = self.s3_client.get_object(\n",
    "                Bucket=bucket_name, Key=os.path.join(prefix, data_file)\n",
    "            )[\"Body\"]\n",
    "            return data_object\n",
    "\n",
    "        def get_model_tokenizer_from_s3(self, bucket_name, prefix, model_name=\"model\"):\n",
    "\n",
    "            if not self.folder_exists_and_not_empty(\n",
    "                bucket_name=bucket_name, key=prefix\n",
    "            ):\n",
    "                model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    \"bert-base-uncased\", num_labels=5\n",
    "                )\n",
    "                tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "                    \"bert-base-uncased\"\n",
    "                )\n",
    "                self.save_model_tokenizer_into_s3(\n",
    "                    bucket_name=bucket_name,\n",
    "                    prefix=prefix,\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                )\n",
    "                return model, tokenizer\n",
    "\n",
    "            else:\n",
    "                tempfile = NamedTemporaryFile()\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/{model_name}.bin\"\n",
    "                ) as f:\n",
    "\n",
    "                    print(\"....\")\n",
    "                    tempfile.write(f.read())\n",
    "\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/config.json\"\n",
    "                ) as f:\n",
    "                    dict_data = json.load(f)\n",
    "                    print(type(dict_data))\n",
    "                    config = PretrainedConfig.from_dict(dict_data)\n",
    "                print(tempfile.name)\n",
    "                #                 model = PreTrainedModel.from_pretrained(\n",
    "                #                     tempfile.name, config=config, num_labels=5\n",
    "                #                 )\n",
    "                model2 = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    tempfile.name, config=config\n",
    "                )\n",
    "                print(\"------>model_done\")\n",
    "                tokenizer = self.get_tokenizer_from_s3(bucket_name, prefix)\n",
    "                return model, tokenizer\n",
    "\n",
    "        def get_tokenizer_from_s3(self, bucket_name, prefix):\n",
    "\n",
    "            tempfile = NamedTemporaryFile()\n",
    "            with self.s3_fileobj(\n",
    "                bucket_name=bucket_name, key=f\"{prefix}/vocab.txt\"\n",
    "            ) as f:\n",
    "                tempfile.write(f.read())\n",
    "            tokenizer = transformers.BertTokenizer.from_pretrained(tempfile.name)\n",
    "            return tokenizer\n",
    "\n",
    "        def save_model_tokenizer_into_s3(\n",
    "            self, bucket_name, prefix, model, model_name, tokenizer=None\n",
    "        ):\n",
    "            buffer = io.BytesIO()\n",
    "            config_string = model.config.to_json_string()\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name, Key=f\"{prefix}/config.json\", Body=config_string\n",
    "            )\n",
    "            torch.save(model.state_dict(), buffer)\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=f\"{prefix}/{model_name}.bin\",\n",
    "                Body=buffer.getvalue(),\n",
    "            )\n",
    "            if tokenizer:\n",
    "                vocab_string = \"\"\n",
    "                for token, token_id in tokenizer.vocab.items():\n",
    "                    vocab_string = vocab_string + token + \"\\n\"\n",
    "\n",
    "                vocab_string = vocab_string.strip()\n",
    "                self.s3_client.put_object(\n",
    "                    Bucket=bucket_name, Key=f\"{prefix}/vocab.txt\", Body=vocab_string\n",
    "                )\n",
    "\n",
    "    s3_adapter_obj = S3Adapter(labels=labels, client=s3_client)\n",
    "    pretrained_model, tokenizer = s3_adapter_obj.get_model_tokenizer_from_s3(\n",
    "        bucket_name=bucket_name, prefix=f\"{prefix}/pretrained\"\n",
    "    )\n",
    "\n",
    "    #     pretrained_model = s3_adapter_obj.get_model_from_s3(\n",
    "    #         bucket_name=bucket_name, prefix=f\"{prefix}/pretrained_model\"\n",
    "    #     )\n",
    "    backbone_model = LabelBackbone(model=pretrained_model)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = AdamW(backbone_model.parameters(), lr=3e-5)\n",
    "\n",
    "    dataframe = pd.read_csv(\n",
    "        s3_adapter_obj.get_data_from_s3(\n",
    "            bucket_name=bucket_name, prefix=prefix, data_file=data_file\n",
    "        )\n",
    "    )\n",
    "    print(dataframe.head())\n",
    "    text = dataframe.loc[:, \"text\"]\n",
    "    label = dataframe.loc[:, \"label\"]\n",
    "\n",
    "    with open(os.path.join(input_dir_path, \"dataset.pkl\"), \"rb\") as f:\n",
    "        train_dataset = dill.load(f)\n",
    "    setattr(train_dataset, \"text\", text)\n",
    "    setattr(train_dataset, \"label\", label)\n",
    "    setattr(train_dataset, \"mode\", \"train\")\n",
    "    setattr(train_dataset, \"tokenizer\", tokenizer)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    try:\n",
    "        backbone_model.to(device)\n",
    "        backbone_model.train()  # Call the train method from the nn.Module base class\n",
    "        print(\"Starting the Training Loop ..\")  # Training loop start\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            train_accuracy = 0\n",
    "            print(f\"[INFO] Epoch {epoch + 1} Started..\")\n",
    "            for index, batch in tqdm(enumerate(train_data_loader)):\n",
    "                print(\n",
    "                    f\"[INFO] [TRAINING] Epoch {epoch + 1} Iteration {index + 1} Running..\"\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "                label = batch[\"label\"].to(device).to(device)\n",
    "                output = backbone_model(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask, labels=label\n",
    "                )\n",
    "                loss = loss_function(output, marker)\n",
    "                test_loss = output[0]\n",
    "                print(\"loss\", test_loss, test_loss.item(), loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = train_loss + loss.item()\n",
    "                _, hypothesis = torch.max(output, dim=1)\n",
    "                train_accuracy = (\n",
    "                    train_accuracy\n",
    "                    + torch.sum(torch.tensor(hypothesis == marker)).item()\n",
    "                )\n",
    "            train_accuracy = train_accuracy / (\n",
    "                len(self.data_loader[0]) * config.params.get(\"train\").get(\"batch_size\")\n",
    "            )\n",
    "            accuracy.append(train_accuracy)\n",
    "            train_loss = train_loss / (\n",
    "                len(self.data_loader[0]) * config.params.get(\"train\").get(\"batch_size\")\n",
    "            )\n",
    "            train_st = f\"Training Loss: {train_loss} Train Accuracy: {train_accuracy}\"\n",
    "            print(f\"Epoch: {epoch+1} {train_st}\")\n",
    "\n",
    "        print(\"Model has been successfully built..\")\n",
    "        # utils_tools.save_model_bin(model_name=config.MARKER_CLASSIFIER, model=self.model)\n",
    "        accuracy = sum(accuracy) / len(accuracy)\n",
    "        s3_adapter_obj.save_model_tokenizer_into_s3(\n",
    "            bucket_name=bucket_name,\n",
    "            prefix=f\"{prefix}/finetuned\",\n",
    "            model=backbone_model.model,\n",
    "        )\n",
    "\n",
    "    except (RuntimeError, MemoryError, ValueError, TypeError) as e:\n",
    "        print(\"Training Exception Occurred\")\n",
    "        raise RuntimeError(\"Training Exception Occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      "<class 'dict'>\n",
      "/tmp/tmpa0dy6u_e\n",
      "------>model_done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlops-kubeflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction with Deadline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnouncement\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOthers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(bucket_name, prefix, data_file, labels, num_epochs, batch_size, device)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3_client\u001b[38;5;241m.\u001b[39mput_object(\n\u001b[1;32m    173\u001b[0m                 Bucket\u001b[38;5;241m=\u001b[39mbucket_name, Key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/vocab.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, Body\u001b[38;5;241m=\u001b[39mvocab_string\n\u001b[1;32m    174\u001b[0m             )\n\u001b[1;32m    176\u001b[0m s3_adapter_obj \u001b[38;5;241m=\u001b[39m S3Adapter(labels\u001b[38;5;241m=\u001b[39mlabels, client\u001b[38;5;241m=\u001b[39ms3_client)\n\u001b[0;32m--> 177\u001b[0m pretrained_model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43ms3_adapter_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_tokenizer_from_s3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/pretrained\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m#     pretrained_model = s3_adapter_obj.get_model_from_s3(\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#         bucket_name=bucket_name, prefix=f\"{prefix}/pretrained_model\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    184\u001b[0m backbone_model \u001b[38;5;241m=\u001b[39m LabelBackbone(model\u001b[38;5;241m=\u001b[39mpretrained_model)\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mtrain.<locals>.S3Adapter.get_model_tokenizer_from_s3\u001b[0;34m(self, bucket_name, prefix, model_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------>model_done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tokenizer_from_s3(bucket_name, prefix)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m, tokenizer\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "mode = \"train\"\n",
    "prefix = \"kf-label-classifier\"\n",
    "bucket_name = \"mlops-kubeflow\"\n",
    "labels = [\n",
    "    \"Action with Deadline\",\n",
    "    \"Announcement\",\n",
    "    \"Appreciation\",\n",
    "    \"Action\",\n",
    "    \"Others\",\n",
    "]\n",
    "    \n",
    "train(\n",
    "    bucket_name=bucket_name,\n",
    "    prefix=prefix,\n",
    "    data_file=\"label.csv\",\n",
    "    labels=labels,\n",
    "    num_epochs=10,\n",
    "    batch_size=2,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7953, 0.3103, 0.1301, 0.9803],\n",
       "        [0.1159, 0.4120, 0.9926, 0.8895],\n",
       "        [0.1988, 0.9261, 0.9853, 0.2093],\n",
       "        [0.2053, 0.1303, 0.6152, 0.2943]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = torch.max(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(index == torch.tensor([1, 2, 3, 4])).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "161277634314.dkr.ecr.us-east-2.amazonaws.com/jupyterlab-base@sha256:1f13f6a1a423739ed6706c53d3759a63d58d428c0cb231d40880c4d598649664",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
