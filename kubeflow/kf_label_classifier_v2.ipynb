{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kfp.components import create_component_from_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"python:3.8-slim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_adapter(bucket_name: str, labels: list, output_dir_path: OutputPath()):\n",
    "\n",
    "    import io\n",
    "    import os\n",
    "    import dill\n",
    "    import boto3\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import transformers\n",
    "    from io import BytesIO\n",
    "    from contextlib import contextmanager\n",
    "    from tempfile import NamedTemporaryFile\n",
    "    from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "    class S3Adapter:\n",
    "        def __init__(self, labels):\n",
    "            self.labels = labels\n",
    "\n",
    "        def folder_exists_and_not_empty(self, bucket_name: str, key: str) -> bool:\n",
    "\n",
    "            if not key.endswith(\"/\"):\n",
    "                key = key + \"/\"\n",
    "            resp = self.s3_client.list_objects(\n",
    "                Bucket=bucket_name, Prefix=key, Delimiter=\"/\", MaxKeys=1\n",
    "            )\n",
    "            return \"Contents\" in resp\n",
    "\n",
    "        @contextmanager\n",
    "        def s3_fileobj(self, bucket_name, key):\n",
    "\n",
    "            \"\"\"\n",
    "            Yields a file object from the filename at {bucket}/{key}\n",
    "\n",
    "            Args:\n",
    "                bucket (str): Name of the S3 bucket where you model is stored\n",
    "                key (str): Relative path from the base of your bucket, including the filename and extension of the object to be retrieved.\n",
    "            \"\"\"\n",
    "\n",
    "            obj = self.s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "            yield BytesIO(obj[\"Body\"].read())\n",
    "\n",
    "        def get_data_from_s3(self, bucket_name, prefix, data_file):\n",
    "\n",
    "            data_object = self.s3_client.get_object(\n",
    "                Bucket=bucket_name, Key=os.path.join(prefix, data_file)\n",
    "            )[\"Body\"]\n",
    "            return data_object\n",
    "\n",
    "        def get_model_tokenizer_from_s3(\n",
    "            self, bucket_name, prefix, model_name=\"pytorch_model\"\n",
    "        ):\n",
    "\n",
    "            if not self.folder_exists_and_not_empty(\n",
    "                bucket_name=bucket_name, key=prefix\n",
    "            ):\n",
    "                model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    \"bert-base-uncased\", num_labels=len(self.labels)\n",
    "                )\n",
    "                tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "                    \"bert-base-uncased\"\n",
    "                )\n",
    "                self.save_model_tokenizer_into_s3(\n",
    "                    bucket_name=bucket_name,\n",
    "                    prefix=prefix,\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                )\n",
    "                return model, tokenizer\n",
    "\n",
    "            else:\n",
    "                tempfile = NamedTemporaryFile()\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/{model_name}.bin\"\n",
    "                ) as f:\n",
    "                    tempfile.write(f.read())\n",
    "\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/config.json\"\n",
    "                ) as f:\n",
    "                    dict_data = json.load(f)\n",
    "                    print(type(dict_data))\n",
    "                    config = PretrainedConfig.from_dict(dict_data)\n",
    "\n",
    "                #                 model = PreTrainedModel.from_pretrained(tempfile.name, config=config)\n",
    "                model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    tempfile.name, config=config\n",
    "                )\n",
    "                tokenizer = self.get_tokenizer_from_s3(bucket_name, prefix)\n",
    "                return model, tokenizer\n",
    "\n",
    "        def get_tokenizer_from_s3(self, bucket_name, prefix):\n",
    "\n",
    "            tempfile = NamedTemporaryFile()\n",
    "            with self.s3_fileobj(\n",
    "                bucket_name=bucket_name, key=f\"{prefix}/vocab.txt\"\n",
    "            ) as f:\n",
    "                tempfile.write(f.read())\n",
    "            tokenizer = transformers.BertTokenizer.from_pretrained(tempfile.name)\n",
    "            return tokenizer\n",
    "\n",
    "        def save_model_tokenizer_into_s3(\n",
    "            self, bucket_name, prefix, model, model_name=\"pytorch_model\", tokenizer=None\n",
    "        ):\n",
    "            buffer = io.BytesIO()\n",
    "            config_string = model.config.to_json_string()\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name, Key=f\"{prefix}/config.json\", Body=config_string\n",
    "            )\n",
    "            torch.save(model.state_dict(), buffer)\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=f\"{prefix}/{model_name}.bin\",\n",
    "                Body=buffer.getvalue(),\n",
    "            )\n",
    "            if tokenizer:\n",
    "                vocab_string = \"\"\n",
    "                for token, token_id in tokenizer.vocab.items():\n",
    "                    vocab_string = vocab_string + token + \"\\n\"\n",
    "\n",
    "                vocab_string = vocab_string.strip()\n",
    "                self.s3_client.put_object(\n",
    "                    Bucket=bucket_name, Key=f\"{prefix}/vocab.txt\", Body=vocab_string\n",
    "                )\n",
    "\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    s3_adpater_obj = S3Adapter(labels=labels)\n",
    "    s3_pickle_path = os.path.join(output_dir_path, \"s3_adapter.pkl\")\n",
    "    with open(s3_pickle_path, \"wb\") as f:\n",
    "        dill.dump(s3_adpater_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(\n",
    "    input_dir_path: InputPath(), labels: list, mode: str, output_dir_path: OutputPath()\n",
    "):\n",
    "\n",
    "    import os\n",
    "    import os\n",
    "    import dill\n",
    "    import torch\n",
    "    import transformers\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class LabelDataset(Dataset):\n",
    "        \"\"\"\n",
    "\n",
    "        LabelDataset class - to load the dataset used the __getitem__ fashion supported by the Pytorch.\n",
    "        The loader supports the JSON and the csv format for parsing the input to the network.\n",
    "            :param mode: mode of Label classifier model (train, eval or serve)\n",
    "            :param text: input text for train, eval and serve components\n",
    "            :param label: output label for train and eval components\n",
    "            :param func_test: True for functional testing of package\n",
    "            :param tokenizer: tokenizer from huggingface library\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, labels, mode=\"serve\", max_length=512, text=None, label=None):\n",
    "            self.mode = mode\n",
    "            self.labels = labels\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __getitem__(self, item):\n",
    "            \"\"\"\n",
    "\n",
    "            Returns tokenized tensors for text and label(if mode is train or eval) for the given index.\n",
    "                :param: item: index to fetch the data.\n",
    "                :returns dict: dictionary of tensors containing input_ids, attention_mask,\n",
    "                               token_type_ids and label(output if mode is train or eval)\n",
    "\n",
    "            \"\"\"\n",
    "            if self.mode in [\"train_eval\", \"train\", \"eval\"]:\n",
    "                text = str(self.text[item])\n",
    "                processed_text = \" \".join(text.split())\n",
    "                label = self.label[item]\n",
    "                inputs = self.tokenizer.encode_plus(\n",
    "                    processed_text,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                padding_length = self.max_length - len(inputs.get(\"input_ids\"))\n",
    "                input_ids = inputs.get(\"input_ids\") + ([0] * padding_length)\n",
    "                attention_mask = inputs.get(\"attention_mask\") + ([0] * padding_length)\n",
    "                token_type_ids = inputs.get(\"token_type_ids\") + ([0] * padding_length)\n",
    "                return {\n",
    "                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                    \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "                    \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                    \"label\": torch.tensor(\n",
    "                        self.labels.index(label.strip()), dtype=torch.long\n",
    "                    ),\n",
    "                }\n",
    "            elif self.mode == \"serve\":\n",
    "                text = str(self.text[item])\n",
    "                text = \" \".join(text.split())\n",
    "                inputs = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                )\n",
    "                padding_length = self.max_length - len(inputs.get(\"input_ids\"))\n",
    "                input_ids = inputs.get(\"input_ids\") + ([0] * padding_length)\n",
    "                attention_mask = inputs.get(\"attention_mask\") + ([0] * padding_length)\n",
    "                token_type_ids = inputs.get(\"token_type_ids\") + ([0] * padding_length)\n",
    "                return {\n",
    "                    \"text\": text,\n",
    "                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                    \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "                    \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                }\n",
    "\n",
    "        def __len__(self):\n",
    "            \"\"\"\n",
    "\n",
    "            Returns length of dataset\n",
    "                :returns int: length of dataset\n",
    "\n",
    "            \"\"\"\n",
    "            return len(self.text)\n",
    "\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    with open(os.path.join(input_dir_path, \"s3_adapter.pkl\"), \"rb\") as f:\n",
    "        s3_adapter_obj = dill.load(f)\n",
    "    with open(os.path.join(output_dir_path, \"s3_adapter.pkl\"), \"wb\") as f:\n",
    "        dill.dump(s3_adapter_obj, f)\n",
    "\n",
    "    dataset_object = LabelDataset(mode=mode, labels=labels)\n",
    "    # Writing LabelBackbone class into model.pkl file\n",
    "    with open(os.path.join(output_dir_path, \"dataset.pkl\"), \"wb\") as f:\n",
    "        dill.dump(dataset_object, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_dir_path: InputPath(),\n",
    "    bucket_name: str,\n",
    "    prefix: str,\n",
    "    data_file: str,\n",
    "    labels: list,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    device: str,\n",
    "    output_dir_path: OutputPath(),\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Training method for Label Classifier. Saves the model after training of model is completed\n",
    "        :param num_epochs: Number of epochs for training purpose\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import io\n",
    "    import os\n",
    "    import dill\n",
    "    import json\n",
    "    import boto3\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import transformers\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    from transformers import AdamW\n",
    "    import torch.nn.functional as f\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    accuracy = []\n",
    "    buffer = io.BytesIO()\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    class LabelBackbone(nn.Module):\n",
    "        \"\"\"\n",
    "\n",
    "        LabelBackbone - Backbone model class for Label Classifier\n",
    "            :param model: Transformer model name from config.TRANSFORMER_MODEL_LIST or saved transformer model name\n",
    "            :param tokenizer: tokenizer from huggingface library\n",
    "            :returns Object of LabelBackbone Model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.drop_out = nn.Dropout(p=0.3)\n",
    "\n",
    "        def forward(self, **kwargs):\n",
    "            return self.model(**kwargs)\n",
    "            #             final_output = self.linear(self.drop_out(po))\n",
    "            return final_output\n",
    "\n",
    "    with open(os.path.join(input_dir_path, \"s3_adapter.pkl\"), \"rb\") as f:\n",
    "        s3_adapter_obj = dill.load(f)\n",
    "\n",
    "    setattr(s3_adapter_obj, \"s3_client\", s3_client)\n",
    "    pretrained_model, tokenizer = s3_adapter_obj.get_model_tokenizer_from_s3(\n",
    "        bucket_name=bucket_name, prefix=f\"{prefix}/pretrained\"\n",
    "    )\n",
    "\n",
    "    #     pretrained_model = s3_adapter_obj.get_model_from_s3(\n",
    "    #         bucket_name=bucket_name, prefix=f\"{prefix}/pretrained_model\"\n",
    "    #     )\n",
    "    backbone_model = LabelBackbone(model=pretrained_model)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = AdamW(backbone_model.parameters(), lr=3e-5)\n",
    "\n",
    "    dataframe = pd.read_csv(\n",
    "        s3_adapter_obj.get_data_from_s3(\n",
    "            bucket_name=bucket_name, prefix=prefix, data_file=data_file\n",
    "        )\n",
    "    )\n",
    "    print(dataframe.head())\n",
    "    text = dataframe.loc[:, \"text\"]\n",
    "    label = dataframe.loc[:, \"label\"]\n",
    "\n",
    "    with open(os.path.join(input_dir_path, \"dataset.pkl\"), \"rb\") as f:\n",
    "        train_dataset = dill.load(f)\n",
    "    setattr(train_dataset, \"text\", text)\n",
    "    setattr(train_dataset, \"label\", label)\n",
    "    setattr(train_dataset, \"mode\", \"train\")\n",
    "    setattr(train_dataset, \"tokenizer\", tokenizer)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    try:\n",
    "        backbone_model.to(device)\n",
    "        backbone_model.train()  # Call the train method from the nn.Module base class\n",
    "        print(\"Starting the Training Loop ..\")  # Training loop start\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            train_accuracy = 0\n",
    "            print(f\"[INFO] Epoch {epoch + 1} Started..\")\n",
    "            for index, batch in tqdm(enumerate(train_data_loader)):\n",
    "                print(\n",
    "                    f\"[INFO] [TRAINING] Epoch {epoch + 1} Iteration {index + 1} Running..\"\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "                label = batch[\"label\"].to(device).to(device)\n",
    "                output = backbone_model(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask, labels=label\n",
    "                )\n",
    "                loss = output[0]\n",
    "                print(output[1])\n",
    "                print(\"loss\", loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = train_loss + loss.item()\n",
    "                _, hypothesis = torch.max(output[1], dim=1)\n",
    "                train_accuracy = (\n",
    "                    train_accuracy + torch.sum(torch.tensor(hypothesis == label)).item()\n",
    "                )\n",
    "            train_accuracy = train_accuracy / (len(train_data_loader) * batch_size)\n",
    "            accuracy.append(train_accuracy)\n",
    "            train_loss = train_loss / (len(train_data_loader) * batch_size)\n",
    "            train_st = f\"Training Loss: {train_loss} Train Accuracy: {train_accuracy}\"\n",
    "            print(f\"Epoch: {epoch+1} {train_st}\")\n",
    "            s3_adapter_obj.save_model_tokenizer_into_s3(\n",
    "                bucket_name=bucket_name,\n",
    "                prefix=f\"{prefix}/finetuned\",\n",
    "                model=backbone_model.model,\n",
    "            )\n",
    "        print(\"Model has been successfully built..\")\n",
    "\n",
    "    except (RuntimeError, MemoryError, ValueError, TypeError) as e:\n",
    "        print(\"Training Exception Occurred\")\n",
    "        raise RuntimeError(\"Training Exception Occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_adapter_comp = create_component_from_func(\n",
    "    s3_adapter,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"boto3\", \"dill\", \"pandas\", \"transformers\", \"torch\"],\n",
    ")\n",
    "dataset_comp = create_component_from_func(\n",
    "    create_data_loader,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"torch\", \"dill\", \"transformers\"],\n",
    ")\n",
    "train_comp = create_component_from_func(\n",
    "    train,\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"torch\", \"pandas\", \"transformers\", \"dill\", \"boto3\", \"numpy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"label-classifier-training-pipeline\")\n",
    "def model_pipeline(labels: list):\n",
    "    mode = \"train\"\n",
    "    prefix = \"kf-label-classifier\"\n",
    "    bucket_name = \"mlops-kubeflow\"\n",
    "    labels = [\n",
    "        \"Action with Deadline\",\n",
    "        \"Announcement\",\n",
    "        \"Appreciation\",\n",
    "        \"Action\",\n",
    "        \"Others\",\n",
    "    ]\n",
    "    s3_adapter_task = s3_adapter_comp(bucket_name=bucket_name, labels=labels)\n",
    "    dataset_task = dataset_comp(\n",
    "        input_dir=s3_adapter_task.output, labels=labels, mode=mode\n",
    "    )\n",
    "    train_task = train_comp(\n",
    "        input_dir=dataset_task.output,\n",
    "        bucket_name=bucket_name,\n",
    "        prefix=prefix,\n",
    "        data_file=\"label.csv\",\n",
    "        labels=labels,\n",
    "        num_epochs=10,\n",
    "        batch_size=2,\n",
    "        device=\"cpu\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-workos-slvr.anthem.com/pipeline/#/experiments/details/bc9aff0e-45e7-4459-81a1-5af693dfcdfb\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-workos-slvr.anthem.com/pipeline/#/experiments/details/bc9aff0e-45e7-4459-81a1-5af693dfcdfb\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow-workos-slvr.anthem.com/pipeline/#/runs/details/bc731be4-c0da-4e75-af17-cb3df61184cd\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=bc731be4-c0da-4e75-af17-cb3df61184cd)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"label_classifier\"\n",
    "HOST = \"https://kubeflow-workos-slvr.anthem.com\"\n",
    "namespace = \"chaluvadi-avinash\"\n",
    "session_cookie = \"MTY0NTQyNTI5M3xOd3dBTkRkVVRUTkxTa3RVVkVnM1MxTkhUVTFKVmxCQ05WQXpVVTFFVUVSSE0wdE9RVXRGTlROSVRsQkhRVU5NV2xkWU4wRktSRkU9fDyBx8kiey-TPSimHemde3ySqxXCVyP_8OhlH06bBMu3\"\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\",\n",
    "    cookies=f\"authservice_session={session_cookie}\",\n",
    "    namespace=namespace,\n",
    "    ssl_ca_cert=\"./root.pem\",\n",
    ")\n",
    "experiment = client.create_experiment(name=EXPERIMENT_NAME, namespace=namespace)\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=model_pipeline,\n",
    "    arguments={},\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=namespace,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "bucket = s3.Bucket(\"mlops-kubeflow\")\n",
    "# bucket.objects.filter(Prefix=\"kf-label-classifier\").delete()\n",
    "s3.Bucket(\"mlops-kubeflow\").upload_file(\"label.csv\", \"kf-label-classifier/label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    bucket_name: str,\n",
    "    prefix: str,\n",
    "    data_file: str,\n",
    "    labels: list,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    device: str,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Training method for Label Classifier. Saves the model after training of model is completed\n",
    "        :param num_epochs: Number of epochs for training purpose\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import io\n",
    "    import os\n",
    "    import dill\n",
    "    import json\n",
    "    import boto3\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import transformers\n",
    "    from tqdm import tqdm\n",
    "    import torch.nn as nn\n",
    "    from transformers import AdamW\n",
    "    import torch.nn.functional as f\n",
    "    from torch.utils.data import DataLoader\n",
    "    from contextlib import contextmanager\n",
    "    from tempfile import NamedTemporaryFile\n",
    "    from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    #     os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "    class LabelBackbone(nn.Module):\n",
    "        \"\"\"\n",
    "\n",
    "        LabelBackbone - Backbone model class for Label Classifier\n",
    "            :param model: Transformer model name from config.TRANSFORMER_MODEL_LIST or saved transformer model name\n",
    "            :param tokenizer: tokenizer from huggingface library\n",
    "            :returns Object of LabelBackbone Model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.drop_out = nn.Dropout(p=0.3)\n",
    "\n",
    "        def forward(self, **kwargs):\n",
    "            return self.model(**kwargs)\n",
    "            #             final_output = self.linear(self.drop_out(po))\n",
    "            return final_output\n",
    "\n",
    "    class S3Adapter:\n",
    "        def __init__(self, labels, client):\n",
    "            self.s3_client = client\n",
    "            self.labels = labels\n",
    "\n",
    "        def folder_exists_and_not_empty(self, bucket_name: str, key: str) -> bool:\n",
    "\n",
    "            if not key.endswith(\"/\"):\n",
    "                key = key + \"/\"\n",
    "            resp = self.s3_client.list_objects(\n",
    "                Bucket=bucket_name, Prefix=key, Delimiter=\"/\", MaxKeys=1\n",
    "            )\n",
    "            return \"Contents\" in resp\n",
    "\n",
    "        @contextmanager\n",
    "        def s3_fileobj(self, bucket_name, key):\n",
    "\n",
    "            \"\"\"\n",
    "            Yields a file object from the filename at {bucket}/{key}\n",
    "\n",
    "            Args:\n",
    "                bucket (str): Name of the S3 bucket where you model is stored\n",
    "                key (str): Relative path from the base of your bucket, including the filename and extension of the object to be retrieved.\n",
    "            \"\"\"\n",
    "\n",
    "            obj = self.s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "            yield io.BytesIO(obj[\"Body\"].read())\n",
    "\n",
    "        def get_data_from_s3(self, bucket_name, prefix, data_file):\n",
    "\n",
    "            data_object = self.s3_client.get_object(\n",
    "                Bucket=bucket_name, Key=os.path.join(prefix, data_file)\n",
    "            )[\"Body\"]\n",
    "            return data_object\n",
    "\n",
    "        def get_model_tokenizer_from_s3(self, bucket_name, prefix, model_name=\"model\"):\n",
    "\n",
    "            if not self.folder_exists_and_not_empty(\n",
    "                bucket_name=bucket_name, key=prefix\n",
    "            ):\n",
    "                model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    \"bert-base-uncased\", num_labels=5\n",
    "                )\n",
    "                tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "                    \"bert-base-uncased\"\n",
    "                )\n",
    "                self.save_model_tokenizer_into_s3(\n",
    "                    bucket_name=bucket_name,\n",
    "                    prefix=prefix,\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                )\n",
    "                return model, tokenizer\n",
    "\n",
    "            else:\n",
    "                tempfile = NamedTemporaryFile()\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/{model_name}.bin\"\n",
    "                ) as f:\n",
    "\n",
    "                    tempfile.write(f.read())\n",
    "\n",
    "                with self.s3_fileobj(\n",
    "                    bucket_name=bucket_name, key=f\"{prefix}/config.json\"\n",
    "                ) as f:\n",
    "                    dict_data = json.load(f)\n",
    "                    config = PretrainedConfig.from_dict(dict_data)\n",
    "                #                 model = PreTrainedModel.from_pretrained(\n",
    "                #                     tempfile.name, config=config, num_labels=5\n",
    "                #                 )\n",
    "                model2 = transformers.BertForSequenceClassification.from_pretrained(\n",
    "                    tempfile.name, config=config\n",
    "                )\n",
    "                tokenizer = self.get_tokenizer_from_s3(bucket_name, prefix)\n",
    "                return model, tokenizer\n",
    "\n",
    "        def get_tokenizer_from_s3(self, bucket_name, prefix):\n",
    "\n",
    "            tempfile = NamedTemporaryFile()\n",
    "            with self.s3_fileobj(\n",
    "                bucket_name=bucket_name, key=f\"{prefix}/vocab.txt\"\n",
    "            ) as f:\n",
    "                tempfile.write(f.read())\n",
    "            tokenizer = transformers.BertTokenizer.from_pretrained(tempfile.name)\n",
    "            return tokenizer\n",
    "\n",
    "        def save_model_tokenizer_into_s3(\n",
    "            self, bucket_name, prefix, model, model_name, tokenizer=None\n",
    "        ):\n",
    "            buffer = io.BytesIO()\n",
    "            config_string = model.config.to_json_string()\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name, Key=f\"{prefix}/config.json\", Body=config_string\n",
    "            )\n",
    "            torch.save(model.state_dict(), buffer)\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=f\"{prefix}/{model_name}.bin\",\n",
    "                Body=buffer.getvalue(),\n",
    "            )\n",
    "            if tokenizer:\n",
    "                vocab_string = \"\"\n",
    "                for token, token_id in tokenizer.vocab.items():\n",
    "                    vocab_string = vocab_string + token + \"\\n\"\n",
    "\n",
    "                vocab_string = vocab_string.strip()\n",
    "                self.s3_client.put_object(\n",
    "                    Bucket=bucket_name, Key=f\"{prefix}/vocab.txt\", Body=vocab_string\n",
    "                )\n",
    "\n",
    "    s3_adapter_obj = S3Adapter(labels=labels, client=s3_client)\n",
    "    pretrained_model, tokenizer = s3_adapter_obj.get_model_tokenizer_from_s3(\n",
    "        bucket_name=bucket_name, prefix=f\"{prefix}/pretrained\"\n",
    "    )\n",
    "\n",
    "    #     pretrained_model = s3_adapter_obj.get_model_from_s3(\n",
    "    #         bucket_name=bucket_name, prefix=f\"{prefix}/pretrained_model\"\n",
    "    #     )\n",
    "    backbone_model = LabelBackbone(model=pretrained_model)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = AdamW(backbone_model.parameters(), lr=3e-5)\n",
    "\n",
    "    dataframe = pd.read_csv(\n",
    "        s3_adapter_obj.get_data_from_s3(\n",
    "            bucket_name=bucket_name, prefix=prefix, data_file=data_file\n",
    "        )\n",
    "    )\n",
    "    print(dataframe.head())\n",
    "    text = dataframe.loc[:, \"text\"]\n",
    "    label = dataframe.loc[:, \"label\"]\n",
    "\n",
    "    with open(os.path.join(input_dir_path, \"dataset.pkl\"), \"rb\") as f:\n",
    "        train_dataset = dill.load(f)\n",
    "    setattr(train_dataset, \"text\", text)\n",
    "    setattr(train_dataset, \"label\", label)\n",
    "    setattr(train_dataset, \"mode\", \"train\")\n",
    "    setattr(train_dataset, \"tokenizer\", tokenizer)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    try:\n",
    "        backbone_model.to(device)\n",
    "        backbone_model.train()  # Call the train method from the nn.Module base class\n",
    "        print(\"Starting the Training Loop ..\")  # Training loop start\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            train_accuracy = 0\n",
    "            print(f\"[INFO] Epoch {epoch + 1} Started..\")\n",
    "            for index, batch in tqdm(enumerate(train_data_loader)):\n",
    "                print(\n",
    "                    f\"[INFO] [TRAINING] Epoch {epoch + 1} Iteration {index + 1} Running..\"\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "                label = batch[\"label\"].to(device).to(device)\n",
    "                output = backbone_model(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask, labels=label\n",
    "                )\n",
    "                loss = loss_function(output, marker)\n",
    "                test_loss = output[0]\n",
    "                print(\"loss\", test_loss, test_loss.item(), loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = train_loss + loss.item()\n",
    "                _, hypothesis = torch.max(output, dim=1)\n",
    "                train_accuracy = (\n",
    "                    train_accuracy\n",
    "                    + torch.sum(torch.tensor(hypothesis == marker)).item()\n",
    "                )\n",
    "            train_accuracy = train_accuracy / (\n",
    "                len(self.data_loader[0]) * config.params.get(\"train\").get(\"batch_size\")\n",
    "            )\n",
    "            accuracy.append(train_accuracy)\n",
    "            train_loss = train_loss / (\n",
    "                len(self.data_loader[0]) * config.params.get(\"train\").get(\"batch_size\")\n",
    "            )\n",
    "            train_st = f\"Training Loss: {train_loss} Train Accuracy: {train_accuracy}\"\n",
    "            print(f\"Epoch: {epoch+1} {train_st}\")\n",
    "\n",
    "        print(\"Model has been successfully built..\")\n",
    "        # utils_tools.save_model_bin(model_name=config.MARKER_CLASSIFIER, model=self.model)\n",
    "        accuracy = sum(accuracy) / len(accuracy)\n",
    "        s3_adapter_obj.save_model_tokenizer_into_s3(\n",
    "            bucket_name=bucket_name,\n",
    "            prefix=f\"{prefix}/finetuned\",\n",
    "            model=backbone_model.model,\n",
    "        )\n",
    "\n",
    "    except (RuntimeError, MemoryError, ValueError, TypeError) as e:\n",
    "        print(\"Training Exception Occurred\")\n",
    "        raise RuntimeError(\"Training Exception Occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "prefix = \"kf-label-classifier\"\n",
    "bucket_name = \"mlops-kubeflow\"\n",
    "labels = [\n",
    "    \"Action with Deadline\",\n",
    "    \"Announcement\",\n",
    "    \"Appreciation\",\n",
    "    \"Action\",\n",
    "    \"Others\",\n",
    "]\n",
    "    \n",
    "train(\n",
    "    bucket_name=bucket_name,\n",
    "    prefix=prefix,\n",
    "    data_file=\"label.csv\",\n",
    "    labels=labels,\n",
    "    num_epochs=10,\n",
    "    batch_size=2,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "161277634314.dkr.ecr.us-east-2.amazonaws.com/jupyterlab-base@sha256:1f13f6a1a423739ed6706c53d3759a63d58d428c0cb231d40880c4d598649664",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
