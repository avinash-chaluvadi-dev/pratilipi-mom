{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name ='demo-kfp-model-training'\n",
    "import random\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import create_component_from_func\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"python:3.8-slim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from minio\n",
    "def download_data(datset:str, output_dir_path: OutputPath()):\n",
    "    from  minio import Minio\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import urllib3\n",
    "    \n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    file_path = os.path.join(output_dir_path, \"data\" + '.csv')\n",
    "    print(\"file_path:\",file_path)\n",
    "\n",
    "    minio_client = Minio(\n",
    "    \"10.110.71.235:9000\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"minio123\",\n",
    "    secure=False,\n",
    "    http_client=urllib3.ProxyManager(\n",
    "        \"http://10.110.71.235:9000/\",\n",
    "        timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n",
    "        cert_reqs=\"CERT_REQUIRED\",\n",
    "        retries=urllib3.Retry(\n",
    "            total=5,\n",
    "            backoff_factor=0.2,\n",
    "            status_forcelist=[500, 502, 503, 504],\n",
    "        ),\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    obj = minio_client.get_object(\"dataset\",datset)\n",
    "    df = pd.read_csv(obj)\n",
    "    print(df.head())\n",
    "    df.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis \n",
    "\n",
    "def explore_data_analysis(input_dir_path: InputPath(), output_dir_path: OutputPath()):\n",
    "    import os\n",
    "    import pandas as pd  \n",
    "    \n",
    "    print(\"input_dir_path: \", input_dir_path)\n",
    "    dir_items = os.listdir(input_dir_path)\n",
    "    print(\"dir_items: \", dir_items)\n",
    "    \n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    file_path = os.path.join(output_dir_path, \"accuracy\" + '.csv')\n",
    "    \n",
    "    input_file_path = input_dir_path + \"/\" + dir_items[0]\n",
    "    \n",
    "    print(\"file_path:\",file_path)\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    print(df.head())\n",
    "\n",
    "    chest_pain=pd.get_dummies(df['cp'],prefix='cp',drop_first=True)\n",
    "    df=pd.concat([df,chest_pain],axis=1)\n",
    "    df.drop(['cp'],axis=1,inplace=True)\n",
    "    sp=pd.get_dummies(df['slope'],prefix='slope')\n",
    "    th=pd.get_dummies(df['thal'],prefix='thal')\n",
    "    rest_ecg=pd.get_dummies(df['restecg'],prefix='restecg')\n",
    "    frames=[df,sp,th,rest_ecg]\n",
    "    df=pd.concat(frames,axis=1)\n",
    "    df.drop(['slope','thal','restecg'],axis=1,inplace=True)\n",
    "\n",
    "    df.to_csv(file_path)\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classification_model(input_dir_path: InputPath(), output_dir_path: OutputPath()):\n",
    "#     import pandas as pd  \n",
    "#     from tensorflow.keras.models import Sequential\n",
    "#     from tensorflow.keras.layers import Dense\n",
    "# #     import keras\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     import os\n",
    "\n",
    "#     print(\"input_dir_path: \", input_dir_path)\n",
    "#     dir_items = os.listdir(input_dir_path)\n",
    "#     print(\"dir_items: \", dir_items)\n",
    "    \n",
    "#     os.makedirs(output_dir_path, exist_ok=True)\n",
    "    \n",
    "#     file_path = os.path.join(output_dir_path, \"classifier\" + '.json')\n",
    "    \n",
    "#     input_file_path = os.path.join(input_dir_path ,dir_items[0])\n",
    "    \n",
    "#     df = pd.read_csv(input_file_path)\n",
    "\n",
    "#     X = df.drop(['target'], axis = 1)\n",
    "#     y = df.target.values\n",
    "#     sc = StandardScaler()\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,  random_state = 20)\n",
    "#     x_train = sc.fit_transform(x_train)\n",
    "#     x_test = sc.transform(x_test)\n",
    "\n",
    "#     classifier = Sequential()\n",
    "#     classifier.add(Dense(units = 11,  activation = 'relu', input_dim = 22))\n",
    "#     classifier.add(Dense(units = 11, activation = 'relu'))\n",
    "#     classifier.add(Dense(units = 1,  activation = 'sigmoid'))\n",
    "#     classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "#     classifier.fit(x_train, y_train, batch_size = 10, epochs = 1)\n",
    "  \n",
    "#     # serialize model to JSON\n",
    "#     model_json = classifier.to_json()\n",
    "    \n",
    "#     with open(file_path, \"w\") as json_file:\n",
    "#         json_file.write(model_json)\n",
    "\n",
    "#     # serialize weights to HDF5\n",
    "#     classifier.save_weights(\"classifier.h5\")\n",
    "#     print(\"Saved model to disk\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model(input_dir_path: InputPath(), output_dir_path: OutputPath()):\n",
    "    import pandas as pd  \n",
    "    from sklearn.linear_model import  LogisticRegression\n",
    "#     import keras\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    print(\"input_dir_path: \", input_dir_path)\n",
    "    dir_items = os.listdir(input_dir_path)\n",
    "    print(\"dir_items: \", dir_items)\n",
    "    \n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(output_dir_path, \"lrmodel\" + '.pkl')\n",
    "    \n",
    "    input_file_path = os.path.join(input_dir_path ,dir_items[0])\n",
    "    \n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    X = df.drop(['target'], axis = 1)\n",
    "    y = df.target.values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,  random_state = 20)\n",
    "    \n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(x_train,y_train)\n",
    "    \n",
    "    pickle.dump(classifier, open(file_path, 'wb'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_knn_model(input_dir_path: InputPath(), output_dir_path: OutputPath()):\n",
    "    import pandas as pd  \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    print(\"input_dir_path: \", input_dir_path)\n",
    "    dir_items = os.listdir(input_dir_path)\n",
    "    print(\"dir_items: \", dir_items)\n",
    "    \n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(output_dir_path, \"knnmodel\" + '.pkl')\n",
    "    \n",
    "    input_file_path = os.path.join(input_dir_path ,dir_items[0])\n",
    "    \n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    X = df.drop(['target'], axis = 1)\n",
    "    y = df.target.values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,  random_state = 20)\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(x_train,y_train)\n",
    "    pickle.dump(knn, open(file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the python function to component\n",
    "download_data_opp = create_component_from_func(download_data,base_image=BASE_IMAGE, packages_to_install=['minio', 'pandas'])\n",
    "\n",
    "eda_opp = create_component_from_func(explore_data_analysis,base_image=BASE_IMAGE, packages_to_install=['pandas'])\n",
    "\n",
    "# classification_model_opp = create_component_from_func(classifier_model,base_image=BASE_IMAGE, packages_to_install=['pandas','sklearn'] )\n",
    "\n",
    "# classifier_model_knn_opp = create_component_from_func(classifier_knn_model,base_image=BASE_IMAGE, packages_to_install=['pandas','sklearn'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name='model-training-pipeline')\n",
    "def model_pipeline(data=\"heart.csv\"):\n",
    "    download_data_task = download_data_opp(data)\n",
    "    eda_opp_task = eda_opp(input_dir=download_data_task.output)\n",
    "    \n",
    "#     classification_model_task = classification_model_opp(input_dir=eda_opp_task.output)\n",
    "#     classifier_model_knn_opp(input_dir=eda_opp_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/676bbb4b-5912-4aa9-a76b-b0a1e334a906\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/166c56a1-a696-4cc6-8eb7-fdcee9bf137d\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=166c56a1-a696-4cc6-8eb7-fdcee9bf137d)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(model_pipeline, 'model-training-pipeline.yaml')\n",
    "\n",
    "#Submit a pipeline run\n",
    "kfp_endpoint = None\n",
    "kfp.Client(host=kfp_endpoint).create_run_from_pipeline_func(model_pipeline,arguments={},experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
